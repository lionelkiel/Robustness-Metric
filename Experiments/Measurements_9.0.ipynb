{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T10:47:11.913037700Z",
     "start_time": "2024-04-16T10:47:05.017579100Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x12dff52c390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "from Helper.ImportDatasetsFOMLAS import df_epsilon, df_epsilon_crit\n",
    "from Helper.ImportDatasetsFairness import df_epsilon as df_epsilon_2\n",
    "from Helper.ImportDatasetsFairness import df_epsilon_crit as df_epsilon_crit_2\n",
    "\n",
    "# drop mnist-net, mnist-net_256x4 and mnist_relu_4_1024 from first df's\n",
    "df_epsilon = df_epsilon[df_epsilon['network'] != 'mnist-net']\n",
    "df_epsilon = df_epsilon[df_epsilon['network'] != 'mnist-net_256x4']\n",
    "df_epsilon = df_epsilon[df_epsilon['network'] != 'mnist_relu_4_1024']\n",
    "\n",
    "df_epsilon_crit = df_epsilon_crit[df_epsilon_crit['network'] != 'mnist-net']\n",
    "df_epsilon_crit = df_epsilon_crit[df_epsilon_crit['network'] != 'mnist-net_256x4']\n",
    "df_epsilon_crit = df_epsilon_crit[df_epsilon_crit['network'] != 'mnist_relu_4_1024']\n",
    "\n",
    "# concat\n",
    "df_epsilon = pd.concat([df_epsilon_2, df_epsilon])\n",
    "df_epsilon_crit = pd.concat([df_epsilon_crit_2, df_epsilon_crit])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9c088319e0ed47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T20:34:08.912200400Z",
     "start_time": "2024-03-30T20:34:08.730457900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "distributions_df_1 = pd.read_pickle('Datasets/distributions_fairness.pkl')\n",
    "distributions_df_2 = pd.read_pickle('Datasets/distributions_fomlas.pkl')\n",
    "\n",
    "distributions_df = pd.concat([distributions_df_1, distributions_df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb7d9a3d8782d97b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T20:34:10.264733300Z",
     "start_time": "2024-03-30T20:34:09.951569500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "\n",
    "def binomial(n, p, x):\n",
    "    '''\n",
    "    :param n: number of trials\n",
    "    :param p: probability of success, value of a (quantile)\n",
    "    :param x: number of successes\n",
    "\n",
    "    :return: probability of x successes\n",
    "    '''\n",
    "\n",
    "    return comb(n, x) * (p ** x) * ((1 - p) ** (n - x))\n",
    "\n",
    "def binomial_bounds(n, p, alpha):\n",
    "    '''\n",
    "    :param n: number of trials\n",
    "    :param p: probability of success, value of a (quantile)\n",
    "    :param alpha: confidence interval\n",
    "\n",
    "    :return: lower and upper bound of confidence interval\n",
    "    '''\n",
    "    probs = np.arange(0, n + 1)\n",
    "    probs = binomial(n, p, probs)\n",
    "\n",
    "    # take sum of probabilities until we reach alpha/2\n",
    "    cumulated_probs = np.cumsum(probs)\n",
    "    lower_index = np.where(cumulated_probs <= alpha / 2)[0][-1]\n",
    "    upper_index = np.where(cumulated_probs >= 1 - alpha / 2)[0][0]\n",
    "\n",
    "    return lower_index, upper_index\n",
    "\n",
    "def get_quantile(network, sigma):\n",
    "    '''\n",
    "    :param network: name of network\n",
    "    :param sigma: quantile\n",
    "\n",
    "    :return: confidence interval for sigma quantile\n",
    "    '''\n",
    "\n",
    "    # Take all critical epsilons of the test set and put into numpy array\n",
    "    df_for_network = df_epsilon_crit[df_epsilon_crit['network'] == network]\n",
    "    df_for_network = df_for_network[df_for_network['ds'] == 'test']\n",
    "    # remove nans\n",
    "    df_for_network = df_for_network.dropna()\n",
    "    crit_epsilons = df_for_network['Epsilon'].to_numpy()\n",
    "\n",
    "    \n",
    "    n = len(crit_epsilons)\n",
    "\n",
    "    # We sort the critical epsilons\n",
    "    order_statistics = np.sort(crit_epsilons)\n",
    "    # We use the order statistics to estimate the sigma quantile\n",
    "    index = int(n * sigma) + 1  # As given by David et al. 1986\n",
    "    lower_index, upper_index = binomial_bounds(n, sigma, 0.05)\n",
    "    return order_statistics[index], order_statistics[lower_index], order_statistics[upper_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b637cb4b1b48340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T20:36:10.122011200Z",
     "start_time": "2024-03-30T20:36:09.162815500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make 95% confidence intervals from the distributions\n",
    "conf_intervals = np.tile(0., (len(networks), 100, 2))\n",
    "\n",
    "for net,network in enumerate(networks):\n",
    "    for run in range(100):\n",
    "        final_bins = distributions_df[distributions_df['network'] == network]['bins'].iloc[run]\n",
    "        final_distribution = distributions_df[distributions_df['network'] == network]['distribution'].iloc[run]\n",
    "        cumsum = np.cumsum(final_distribution)\n",
    "        lower_index = np.where(cumsum <= 0.025)[0][-1]\n",
    "        upper_index = np.where(cumsum >= 0.975)[0][0]\n",
    "        \n",
    "        bin_size = final_bins[1] - final_bins[0]\n",
    "        \n",
    "        conf_intervals[net, run, 0] = final_bins[lower_index]\n",
    "        conf_intervals[net, run, 1] = final_bins[upper_index] + bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58a964eae9fdbd3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T21:02:45.876264Z",
     "start_time": "2024-03-30T21:02:42.981082Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the quantiles for the networks\n",
    "networks = distributions_df['network'].unique()\n",
    "\n",
    "in_confidence_interval = np.tile(0, (len(networks), 100))\n",
    "overlap_prob = np.tile(0., (len(networks), 100))\n",
    "overlap_size = np.tile(0., (len(networks), 100))\n",
    "\n",
    "for net,network in enumerate(networks):\n",
    "    for run in range(100):\n",
    "        final_bins = distributions_df[distributions_df['network'] == network]['bins'].iloc[run]\n",
    "        final_distribution = distributions_df[distributions_df['network'] == network]['distribution'].iloc[run]\n",
    "        quantile, lower, upper = get_quantile(network, 0.05)\n",
    "        upper = upper + 0.002\n",
    "        \n",
    "        lower_bound_area = conf_intervals[net, run, 0]\n",
    "        upper_bound_area = conf_intervals[net, run, 1]\n",
    "        \n",
    "        if lower_bound_area < upper <= upper_bound_area:\n",
    "            metric = 1\n",
    "        elif lower_bound_area <= lower < upper_bound_area:\n",
    "            metric = 1\n",
    "        elif lower <= lower_bound_area and upper >= upper_bound_area:\n",
    "            metric = 1\n",
    "        else:\n",
    "            metric = 0\n",
    "        \n",
    "        in_confidence_interval[net, run] = metric\n",
    "        \n",
    "        # Metric 2, probability given to the area\n",
    "        if metric == 1:\n",
    "            lower_bound_index = torch.where(final_bins >= lower)[0]\n",
    "            lower_bound_index = lower_bound_index[0]\n",
    "    \n",
    "            upper_bound_index = torch.where(final_bins + bin_size <= upper)[\n",
    "                0]  # we don't include bins who's right side is larger than the quantile\n",
    "            upper_bound_index = upper_bound_index[-1]\n",
    "    \n",
    "            metric_2 = torch.sum(final_distribution[lower_bound_index:upper_bound_index + 1]).item()\n",
    "    \n",
    "        else:\n",
    "            metric_2 = 0\n",
    "        \n",
    "        overlap_prob[net, run] = metric_2\n",
    "        # Metric 3, Area of overlap\n",
    "        if metric == 1:\n",
    "            metric_3 = final_bins[upper_bound_index] - final_bins[lower_bound_index] + bin_size\n",
    "\n",
    "        else:\n",
    "            metric_3 = 0\n",
    "        \n",
    "        size = final_bins[-1] - final_bins[0] + bin_size\n",
    "        overlap_size[net, run] = metric_3/size\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19991c698e0683",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79292ce1eee31d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T21:03:05.555826900Z",
     "start_time": "2024-03-30T21:03:05.536819200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52189523, 0.42554799, 0.48668227, ..., 0.42915717, 0.50175625,\n",
       "        0.42919394],\n",
       "       [0.        , 0.41917801, 0.28487223, ..., 0.28350392, 0.        ,\n",
       "        0.25827324],\n",
       "       [0.34561378, 0.48164874, 0.22800662, ..., 0.2279824 , 0.32374328,\n",
       "        0.33041891],\n",
       "       ...,\n",
       "       [0.        , 0.90045929, 0.78100729, ..., 0.83494377, 0.66973722,\n",
       "        0.5894286 ],\n",
       "       [0.79032296, 0.56099623, 0.54119807, ..., 0.86089855, 0.62486941,\n",
       "        0.42090073],\n",
       "       [0.64175224, 0.89527428, 0.82082838, ..., 0.81492001, 0.7960645 ,\n",
       "        0.78606957]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "387b3a3d7d73af44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T21:12:03.849988700Z",
     "start_time": "2024-03-30T21:12:03.769913800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m total \u001b[38;5;241m=\u001b[39m in_confidence_interval\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m average_prob \u001b[38;5;241m=\u001b[39m overlap_prob[overlap_prob\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m average_size \u001b[38;5;241m=\u001b[39m overlap_size\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# make latex table\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\RobustnessMetric\\Lib\\site-packages\\numpy\\core\\_methods.py:106\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    102\u001b[0m arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[0;32m    104\u001b[0m is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m rcount \u001b[38;5;241m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    108\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\RobustnessMetric\\Lib\\site-packages\\numpy\\core\\_methods.py:77\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[1;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[0;32m     75\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[1;32m---> 77\u001b[0m         items \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[mu\u001b[38;5;241m.\u001b[39mnormalize_axis_index(ax, arr\u001b[38;5;241m.\u001b[39mndim)]\n\u001b[0;32m     78\u001b[0m     items \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mintp(items)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "total = in_confidence_interval.sum(axis=1)\n",
    "average_prob = overlap_prob.mean(axis=1)\n",
    "average_size = overlap_size.mean(axis=1)\n",
    "\n",
    "# make latex table\n",
    "\n",
    "for net,network in enumerate(networks):\n",
    "    name = network.replace('_',' ')\n",
    "    print(f'{name} & {total[net]} & {average_prob[net]:.2f} & {average_size[net]:.2f} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23becd5c74cac191",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
